# Ansible → AWS artifact deployment guide

This document explains how to write Ansible playbooks and roles to pull deployment artifacts from AWS (S3 / CodePipeline artifacts) and deploy them to AWS targets (EC2, or by triggering AWS CodeDeploy). It contains practical examples and security recommendations.

What I did
- Wrote a single Markdown guide with clear prerequisites, step-by-step examples, and two concrete playbooks you can adapt.
- Included a role layout example and variables to keep playbooks reusable.
- Added notes on credentials (env, profiles, IAM roles, Ansible Vault) and testing/troubleshooting tips.

What you can do next
- Pick the example that matches your artifact location (S3 or CodeDeploy).
- Adjust variables (bucket/key/service names, deploy paths) to match your app.
- If you want, I can convert the example into a ready-to-run role and CI pipeline (GitHub Actions) in the repo.

---

## Table of contents
- Overview
- Prerequisites
- Authentication & security
- Recommended role layout
- Example A — Pull artifact from S3 and deploy to EC2
  - Playbook
  - How it works
  - Run example
- Example B — Trigger CodeDeploy deployment (when you use AWS CodeDeploy)
  - Playbook
  - How it works
  - Run example
- Best practices & idempotence
- Troubleshooting

---

## Overview
This guide focuses on typical deployment steps:
1. Fetch artifact from AWS (S3 / CodePipeline).
2. Place artifact on target host(s).
3. Unpack, install or update symlink, run migrations if needed.
4. Restart services or trigger managed deployment (CodeDeploy).

It is written to be cloud-idiomatic: prefer IAM roles for instances or ephemeral credentials, and keep secrets in Ansible Vault or environment variables.

---

## Prerequisites
- Ansible 2.10+ (collections model). Newer is better.
- Python and boto3 installed on the controller machine (the machine running Ansible) and, if modules require, also on hosts that use boto modules.
- Install the Amazon collection:
  - pip install boto3 botocore
  - ansible-galaxy collection install amazon.aws
- AWS CLI (useful for some examples) installed and available on the controller or target hosts when used in shell tasks.
- Access to the target EC2 hosts (SSH keys / inventory).
- For S3 access: permission to GetObject on the bucket.
- For CodeDeploy: permissions to create deployments and read apps/deployment groups.

---

## Authentication & security
- Preferred: run playbooks from a machine that has an IAM role (EC2) or use AWS profiles with limited scope.
- Alternate: set AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY (and optionally AWS_SESSION_TOKEN) in controller environment or in the playbook via lookup of an Ansible Vault secret.
- Use Ansible Vault for long-lived secrets:
  - ansible-vault encrypt_string 'AKIA...' --name 'aws_access_key'
- Never hardcode credentials in playbooks or repo.

Example environment:
- export AWS_PROFILE=deploy-profile
- or configure ~/.aws/credentials with a named profile.

---

## Recommended role layout
A reusable role makes deploying multiple applications easier:

roles/
  app_deploy/
    tasks/
      main.yml
      fetch_artifact_s3.yml
      extract.yml
      symlink.yml
    handlers/
      main.yml
    defaults/
      main.yml
    vars/
      main.yml

Key variables:
- s3_bucket
- s3_key
- artifact_name (optional; autogenerated from s3_key)
- deploy_dir (where current release will live)
- releases_dir (for release-based deploys)
- keep_releases (for cleanup)
- service_name (systemd service to restart)

---

## Example A — Pull artifact from S3 and deploy to EC2
This example downloads an artifact (zip/tar) from S3, extracts it into a releases directory, updates a `current` symlink, and restarts a service.

Playbook: (single-playbook example)
```yaml
---
- name: Deploy app from S3 to app servers
  hosts: app_servers
  become: true
  vars:
    s3_bucket: my-deploy-bucket
    s3_key: artifacts/my-app-1.2.3.tar.gz   # path in bucket
    deploy_user: deploy
    releases_dir: /opt/myapp/releases
    current_path: /opt/myapp/current
    keep_releases: 5
    service_name: myapp
    tmp_artifact: /tmp/{{ (s3_key | basename) }}
  collections:
    - amazon.aws

  tasks:
    - name: Ensure required packages (unarchive depends on bsdtar/tar)
      ansible.builtin.package:
        name: "{{ item }}"
        state: present
      loop:
        - tar
        - gzip

    - name: Ensure releases dir exists
      ansible.builtin.file:
        path: "{{ releases_dir }}"
        state: directory
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"
        mode: '0755'

    - name: Download artifact from S3
      amazon.aws.aws_s3:
        bucket: "{{ s3_bucket }}"
        object: "{{ s3_key }}"
        dest: "{{ tmp_artifact }}"
        mode: get
      register: s3_get_result

    - name: Create a release directory for this deployment
      ansible.builtin.set_fact:
        release_timestamp: "{{ lookup('pipe','date +%Y%m%d%H%M%S') }}"
    - name: Make release directory
      ansible.builtin.file:
        path: "{{ releases_dir }}/{{ release_timestamp }}"
        state: directory
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"
        mode: '0755'

    - name: Extract artifact into new release dir
      ansible.builtin.unarchive:
        src: "{{ tmp_artifact }}"
        dest: "{{ releases_dir }}/{{ release_timestamp }}"
        remote_src: yes
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"

    - name: Update current symlink to new release
      ansible.builtin.file:
        src: "{{ releases_dir }}/{{ release_timestamp }}"
        dest: "{{ current_path }}"
        state: link
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"

    - name: Ensure permissions are correct (recursive)
      ansible.builtin.file:
        path: "{{ current_path }}"
        recurse: yes
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"

    - name: Restart application service
      ansible.builtin.systemd:
        name: "{{ service_name }}"
        state: restarted
        enabled: yes

    - name: Cleanup old releases (keep last N)
      ansible.builtin.shell: |
        ls -1dt {{ releases_dir }}/* | tail -n +{{ keep_releases + 1 }} | xargs -r rm -rf
      args:
        warn: false
      changed_when: false
      # optionally register and set changed_when appropriately
```

How it works:
- Uses amazon.aws.aws_s3 module to fetch the object from S3 into /tmp on the remote target.
- Extracts into a timestamped release directory, updates a `current` symlink so rollbacks are simple.
- Restarts the systemd service.

How to run:
- Inventory example (inventory.ini):
  ```
  [app_servers]
  ec2-1.example.com ansible_user=ec2-user
  ec2-2.example.com ansible_user=ec2-user
  ```
- Run:
  - With env profile: AWS_PROFILE=deploy-profile ansible-playbook -i inventory.ini deploy-s3.yml
  - Or ensure permissions via instance role.

Notes:
- If you prefer controller to download artifact and push to hosts, change mode/get to mode=control and dest on controller, then use copy/assemble to push. However remote download is usually faster for each host (if running inside same VPC).

---

## Example B — Trigger CodeDeploy (for centralized deployment)
If your pipeline builds and uploads artifacts and registers a deployment in AWS CodeDeploy, you can have Ansible trigger a deployment instead of doing file operations on instances.

This example shows using the AWS CLI to create a deployment. You can also use the aws boto3 call via an Ansible module or a custom module, but aws cli is straightforward.

Playbook:
```yaml
---
- name: Trigger AWS CodeDeploy deployment
  hosts: localhost
  gather_facts: false
  vars:
    application_name: my-app
    deployment_group_name: my-deployment-group
    revision_s3_bucket: my-deploy-bucket
    revision_s3_key: artifacts/my-app-1.2.3.zip
    deployment_description: "Ansible triggered deployment for version 1.2.3"
  tasks:
    - name: Create CodeDeploy deployment
      ansible.builtin.shell: >
        aws deploy create-deployment
          --application-name {{ application_name }}
          --deployment-group-name {{ deployment_group_name }}
          --description "{{ deployment_description }}"
          --s3-location bucket={{ revision_s3_bucket }},key={{ revision_s3_key }},bundleType=zip
      register: create_deploy
    - name: Show deployment id
      ansible.builtin.debug:
        msg: "Deployment created: {{ create_deploy.stdout }}"
```

How it works:
- Runs on localhost (controller) and calls `aws deploy create-deployment` to start a managed CodeDeploy deployment.
- CodeDeploy agents on target EC2 instances handle artifact download and lifecycle hooks (AppSpec) defined in the artifact.

Notes:
- Make sure the artifact contains an appspec.yml (CodeDeploy) and that deployment group IAM roles are set correctly.
- You can poll deployment status via `aws deploy get-deployment` and fail/play until it completes.

---

## Idempotence & testing
- Use symlink + releases pattern to make reverts easy and to keep deploys idempotent.
- Use handlers (notify) for service restarts triggered only when code changed.
- Test with a staging inventory and one server before rolling out to production.
- Use --check and --diff for dry-run insights where safe.

---

## Best practices
- Prefer remote fetch (S3 -> target) if bandwidth/localization makes sense.
- Use instance IAM roles (least privilege) rather than embedding credentials.
- Store secrets in Ansible Vault.
- Keep playbooks small and composable (roles).
- Use CI to generate the S3 key (release artifact) and pass that as variable to Ansible (e.g., via environment variable or extra-vars).

Example CI invocation:
- ansible-playbook -i prod.ini deploy-s3.yml -e "s3_key=artifacts/my-app-1.2.4.tar.gz"

---

## Troubleshooting
- "AccessDenied" from aws_s3: check IAM permissions for GetObject and bucket policy.
- Missing boto3: install boto3 on controller (pip install boto3).
- Timeouts on large files: use pre-signed URL + get_url if better for your environment.
- Verify CodeDeploy agent logs on the instance at /var/log/aws/codedeploy-agent/codedeploy-agent.log

---

If you'd like I can:
- Convert the S3 example into a reusable role (roles/app_deploy) with tests.
- Provide a GitHub Actions workflow that runs the playbook and passes artifact s3_key automatically from your build pipeline.
