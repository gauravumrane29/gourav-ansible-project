```markdown
# Corporate-grade Ansible → AWS Deployment Guide with Jenkins CI/CD

What I did and where we are now
I expanded the original Ansible + AWS deployment guide into a corporate-standard playbook and CI/CD design using Jenkins. The document below includes governance, security, branching and release policies, observability, testing and approval gates, a full Jenkins pipeline (Jenkinsfile) example, recommended plugins and policies, secrets handling, rollback strategy, and concrete Ansible playbook adjustments for safe automated deployments. Read the "What's next" section at the end for immediate follow-ups you can ask me to produce (e.g., ready-to-run Jenkins job DSL, a fully scaffolded Ansible role, or a shared Jenkins pipeline library).

Table of contents
- Goals & assumptions
- Corporate standard practices (policy + governance)
- Artifact & release management
- CI pipeline design (Jenkins) — stages & responsibilities
- Example Jenkinsfile (declarative pipeline)
- Jenkins job & credentials configuration (how to store secrets)
- CD/Deployment strategies (Ansible integration)
- Ansible playbook adjustments for CI integration
- Testing strategy (unit, integration, e2e, smoke, canary)
- Security scanning & compliance (SCA, SBOM, signing)
- Observability, logging & health checks
- Rollback & disaster recovery
- Recommended plugins & infra for Jenkins
- Appendices: sample CLI commands, environment variables, and housekeeping

---

Goals & assumptions
- Artifacts are packaged by CI and stored in AWS S3 (versioned, encrypted). Alternatively CodeArtifact or an artifact repository can be used.
- Deploy targets are EC2 instances in private subnets that run an application service (systemd) and optionally the CodeDeploy agent.
- Jenkins orchestrates build and deployment; Ansible does the remote configuration and deploy steps (either pulling the artifact from S3 or using an artifact already pushed to the agent).
- Principle of least privilege for all AWS identities; use temporary credentials (IAM role + STS) or Jenkins credentials plugin + Vault.
- Compliance/audit requirements: all actions are logged (CloudTrail + Jenkins audit + Ansible logs).

---

Corporate standard practices (policy + governance)
- Branching & PRs
  - Git trunk-based flow: feature branches -> PR -> CI -> merge to main.
  - Enforce required status checks: unit tests, lint, security scan, build.
  - Require code review: at least 1 approver for non-prod, 2 for production-impacting changes.
  - Protect main: block pushes, require PR merge from authorized users, require passing pipeline.

- Release & versioning
  - Semantic Versioning for releases (MAJOR.MINOR.PATCH).
  - Every release must produce an immutable artifact with a manifest (artifact name, version, S3 key, build metadata, commit SHA, SBOM reference).
  - Sign artifacts (GPG or AWS KMS envelope) where required.

- Approval & change windows
  - Production deployments require:
    - Successful automated checks, and
    - A manual approval (Jenkins input or GitOps promotion) within an allowed change window (configurable).
  - Change windows and emergency change procedures documented and audited.

- Access control & least privilege
  - Use IAM roles for Jenkins agents and EC2 instances.
  - Jenkins store credentials in its credentials store or a secrets manager (HashiCorp Vault, AWS Secrets Manager).
  - Use role assumption (sts:AssumeRole) for cross-account deploys.

- Secrets & keys
  - No secrets in code. Use Ansible Vault OR fetch secrets dynamically from Vault/AWS Secrets Manager at runtime.
  - Rotate keys regularly; record rotation events.

- Audit & logging
  - Enable AWS CloudTrail across accounts and regions.
  - Enable S3 server access logs on artifact buckets and lifecycle rules (retention).
  - Centralized logging for Jenkins (audit trail) and Ansible runs (log to central ELK/CloudWatch).

- Testing & quality gates
  - All PRs must pass unit tests and static analysis.
  - Security scans (SCA) and dependency checks must pass before publish.

- Documentation & runbooks
  - Every deployment type has a runbook for manual rollback, incident contact, and mitigation steps stored in the repo or a central knowledge base.

---

Artifact & release management
- S3 bucket config
  - Enable versioning, block public access, enable SSE-KMS encryption, logging and lifecycle rules.
  - S3 path layout: s3://my-deploy-bucket/<app>/<major>.<minor>/<version>/<artifact>
  - Publish a JSON manifest alongside the artifact:
    {
      "app": "my-app",
      "version": "1.2.3",
      "s3_key": "artifacts/my-app/1.2.3/my-app-1.2.3.tar.gz",
      "commit": "abcdef123456",
      "build": 45,
      "sbom": "s3://my-deploy-bucket/artifacts/my-app/1.2.3/sbom.json",
      "signed": true
    }

- Retention & immutability
  - Use Object Lock (WORM) or lifecycle policy for critical artifacts where regulatory compliance demands immutability.

---

CI pipeline design (Jenkins) — stages & responsibilities
High-level pipeline responsibilities:
1. Checkout -> run linters, static checks
2. Unit tests -> coverage threshold (fail if below)
3. Build -> produce artifact (tar/zip/container image)
4. Security scans -> SCA, dependency vulnerabilities, container scans
5. SBOM generation -> store with artifact
6. Sign artifact -> GPG or KMS
7. Publish -> upload artifact + manifest to S3 (use atomic naming and metadata)
8. Promote -> tag Git commit and create release record
9. Trigger CD -> artifact promotion, or call Ansible deploy job for environment(s)

Environments:
- CI builds for every PR and branch
- CD pipelines for staging/qa (automated)
- Production requires a manual promotion/approval step

Parallelism:
- Run scanning steps in parallel where possible (SAST, SCA, container scans). Fail fast on critical issues.

Stability & reliability:
- Use retry for transient network calls, timeouts per stage, and agent allocation policies.
- Use multibranch pipeline job for feature branches and PRs.

---

Example Jenkinsfile (declarative pipeline)

This Jenkinsfile illustrates a multi-stage pipeline that produces an S3 artifact and triggers an Ansible deployment job. It uses Jenkins credentials (an AWS credential with access/secret or role-based) and an SSH credential for Ansible (ssh private key credential). Production path includes a manual approval.

```groovy
pipeline {
  agent { label 'linux && docker' }
  options {
    ansiColor('xterm')
    buildDiscarder(logRotator(numToKeepStr: '30'))
    timeout(time: 120, unit: 'MINUTES')
    timestamps()
  }
  environment {
    APP = 'my-app'
    S3_BUCKET = 'my-deploy-bucket'
    // Jenkins credentials IDs
    AWS_CREDENTIALS_ID = 'aws-deploy-creds'
    SSH_KEY_CREDENTIALS_ID = 'ansible-deploy-key'
    ARTIFACT_NAME = "${APP}-${env.BUILD_NUMBER}.tar.gz"
    S3_KEY = "artifacts/${APP}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}/${ARTIFACT_NAME}"
    MANIFEST = "manifest-${env.BUILD_NUMBER}.json"
  }

  parameters {
    booleanParam(name: 'PROD_PROMOTE', defaultValue: false, description: 'If true, will trigger a production deploy after staging')
  }

  stages {
    stage('Checkout') {
      steps {
        checkout scm
        sh 'git rev-parse --abbrev-ref HEAD || true'
      }
    }

    stage('Lint & Static Analysis') {
      parallel {
        stage('Ansible Lint') {
          steps {
            sh 'ansible-lint -v || true' // don't fail pipeline for soft lint; configure policy
          }
        }
        stage('Code Lint') {
          steps {
            sh './gradlew check || ./mvnw -DskipTests=false verify || true'
          }
        }
      }
    }

    stage('Unit Tests') {
      steps {
        sh './run-unit-tests.sh' // adapt to repo
        junit '**/build/test-results/**/*.xml'
      }
    }

    stage('Build Artifact') {
      steps {
        sh '''
          mkdir -p dist
          tar -czf dist/${ARTIFACT_NAME} ./app || true
        '''
        stash includes: 'dist/**', name: 'artifact'
      }
      post {
        success { archiveArtifacts artifacts: 'dist/**', fingerprint: true }
      }
    }

    stage('Security Scans & SBOM') {
      parallel {
        stage('Dependency SCA') {
          steps {
            sh 'snyk test || true'
            // optionally fail on policy
          }
        }
        stage('SBOM') {
          steps {
            sh 'syft packages dir:./app -o json > dist/sbom-${BUILD_NUMBER}.json || true'
          }
        }
      }
      post { always { archiveArtifacts artifacts: 'dist/sbom-*.json', fingerprint: true } }
    }

    stage('Sign & Publish Artifact') {
      steps {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', credentialsId: env.AWS_CREDENTIALS_ID]]) {
          sh '''
            # Upload artifact and manifest
            aws s3 cp dist/${ARTIFACT_NAME} s3://${S3_BUCKET}/${S3_KEY} --sse aws:kms
            cat > dist/${MANIFEST} <<EOF
            {
              "app": "${APP}",
              "version": "${BUILD_NUMBER}",
              "s3_key": "${S3_KEY}",
              "commit": "$(git rev-parse --short HEAD)",
              "build": "${BUILD_NUMBER}"
            }
            EOF
            aws s3 cp dist/${MANIFEST} s3://${S3_BUCKET}/${S3_KEY}.manifest.json --sse aws:kms
          '''
        }
      }
    }

    stage('Deploy to Staging') {
      steps {
        // call an Ansible job on Jenkins or run Ansible from this pipeline
        withCredentials([sshUserPrivateKey(credentialsId: env.SSH_KEY_CREDENTIALS_ID, keyFileVariable: 'ANSIBLE_SSH_KEY')]) {
          unstash 'artifact'
          sh '''
            export ANSIBLE_PRIVATE_KEY_FILE=${ANSIBLE_SSH_KEY}
            ansible-playbook -i inventories/staging playbooks/deploy-s3.yml -e "s3_bucket=${S3_BUCKET} s3_key=${S3_KEY} build=${BUILD_NUMBER}" --ssh-extra-args='-o StrictHostKeyChecking=no'
          '''
        }
      }
      post {
        success { echo "Staging deployment successful" }
        failure { error "Staging deployment failed" }
      }
    }

    stage('Run Smoke Tests') {
      steps {
        sh './scripts/run-smoke-tests.sh || true'
        // Evaluate results and fail pipeline if required
      }
    }

    stage('Promote to Prod (Manual)') {
      when {
        expression { params.PROD_PROMOTE == true }
      }
      steps {
        timeout(time: 30, unit: 'MINUTES') {
          input message: "Approve production deployment for build ${env.BUILD_NUMBER}?", ok: 'Deploy to Prod', submitter: 'release-managers'
        }
        withCredentials([sshUserPrivateKey(credentialsId: env.SSH_KEY_CREDENTIALS_ID, keyFileVariable: 'ANSIBLE_SSH_KEY')]) {
          sh '''
            export ANSIBLE_PRIVATE_KEY_FILE=${ANSIBLE_SSH_KEY}
            ansible-playbook -i inventories/prod playbooks/deploy-s3.yml -e "s3_bucket=${S3_BUCKET} s3_key=${S3_KEY} build=${BUILD_NUMBER} deployment_strategy=blue_green" --ssh-extra-args='-o StrictHostKeyChecking=no'
          '''
        }
      }
    }
  }

  post {
    always {
      archiveArtifacts artifacts: 'dist/**', allowEmptyArchive: true
      // notify, e.g., Slack or email (via plugin)
    }
    failure {
      // rollback hooks or create incident
    }
  }
}
```

Notes on the Jenkinsfile
- Use Jenkins agents with the right capabilities (docker, AWS CLI).
- Use Jenkins credentials binding for AWS and SSH. In production, prefer IAM role-based agents instead of static AWS keys.
- The Jenkinsfile demonstrates a manual input gate for production promotion.
- Consider using a shared Jenkins pipeline library for common steps.

---

Jenkins job & credentials configuration (how to store secrets)
- Credentials to create in Jenkins:
  - AWS credentials: use Jenkins "AWS Credentials" plugin or store as username/password type with access_key/secret_key; prefer IAM role with amazon-ec2-instance-profile on Jenkins agents.
  - SSH key: store as "SSH Username with private key" credential (for Ansible).
  - Vault token: if using HashiCorp Vault, use Vault plugin to fetch secrets at runtime.
  - GPG/KMS key references: do not store private keys in Jenkins; use KMS/GCP/KMS-based signing or Vault.
- Permissions
  - Apply Role-Based Authorization Strategy plugin: restrict who can approve builds, who can create jobs, and restrict credentials access.
- Credential usage
  - Use withCredentials() wrapper in pipelines.
  - Do not print secrets to console. Configure pipeline logs to mask values.

---

CD / Deployment strategies (Ansible integration)
- Options
  - Controller-based push: Jenkins pulls artifact then runs ansible-playbook pushing files to hosts (useful if controller has network access).
  - Target pull: Ansible on each host pulls artifact from S3. Requires instance role with S3 read permissions.
  - Managed deploy: Use AWS CodeDeploy for lifecycle hooks and rollback. Ansible triggers CodeDeploy (or vice versa).

- Recommended production strategies
  - Blue/Green:
    - Provision new target group (or instances), deploy the new release, run health checks, switch router/ALB target group.
    - Tear down old group after promotion delay.
  - Rolling with minimum availability:
    - Update hosts in batches, with health checks at each batch.
  - Canary:
    - Deploy to a small subset, run e2e metrics and increase rollout gradually.

- Ansible role responsibilities
  - Ensure idempotence.
  - Use app releases + current symlink pattern.
  - Run DB migrations as an atomic step with a pre/post hook and a maintenance flag.
  - Use handlers to restart services only when necessary.

---

Ansible playbook adjustments for CI integration
- Parameterization
  - Accept variables from Jenkins extra-vars: s3_bucket, s3_key, build, deployment_strategy, release_timestamp.
  - Use --extra-vars in ansible-playbook invocation to pass build metadata.

- Inventory & dynamic hosts
  - Use AWS EC2 dynamic inventory plugin or maintain inventories in Git and have Jenkins select the right inventory file.

- Ansible Vault & secrets
  - For secrets, prefer fetching runtime secrets from Vault by using houston/hashi modules or run a pre-task in the pipeline that injects secrets into a temporary vault file on the Jenkins agent, then run ansible-playbook with --ask-vault-pass or --vault-password-file.

- Example playbook adjustments (snippet)
```yaml
- name: Deploy app from S3 to app servers
  hosts: app_servers
  become: true
  vars:
    s3_bucket: "{{ s3_bucket }}"
    s3_key: "{{ s3_key }}"
    release_timestamp: "{{ lookup('pipe','date +%Y%m%d%H%M%S') if release_timestamp is not defined else release_timestamp }}"
    releases_dir: /opt/myapp/releases
    current_path: /opt/myapp/current
    service_name: myapp
  collections:
    - amazon.aws
  tasks:
    - name: Ensure releases directory exists
      file:
        path: "{{ releases_dir }}"
        state: directory
        owner: app
        group: app
        mode: '0755'

    - name: Download artifact from S3 (remote)
      amazon.aws.aws_s3:
        bucket: "{{ s3_bucket }}"
        object: "{{ s3_key }}"
        dest: "/tmp/{{ s3_key | basename }}"
        mode: get

    - name: Extract artifact
      unarchive:
        src: "/tmp/{{ s3_key | basename }}"
        dest: "{{ releases_dir }}/{{ release_timestamp }}"
        remote_src: yes

    - name: Symlink current to release
      file:
        src: "{{ releases_dir }}/{{ release_timestamp }}"
        dest: "{{ current_path }}"
        state: link
```

Tips:
- If Jenkins runs on a network path with faster download to a controller, set aws_s3 mode=control and then use copy to push to hosts.
- Use retries for transient S3 errors.

---

Testing strategy
- Unit tests: run in CI for code and role logic (molecule for Ansible roles).
- Integration tests: create ephemeral environment (cloud sandbox) and run tests against it.
- Acceptance/e2e tests: run smoke tests post-deploy.
- Pre-production Canary: automatically route a small % of traffic and run deterministic checks for a time window; promote if healthy.
- Performance tests as part of a scheduled pipeline or on-demand.

Suggested tests in pipeline:
- ansible-lint + molecule lint
- molecule: create -> converge -> verify -> destroy (on ephemeral infra)
- Post-deploy smoke tests: endpoint health, DB verification, error rate checks.

---

Security scanning & compliance
- SCA for dependencies: Snyk/OSS Index/OWASP dependency check.
- Container scanning: Trivy/Clair/Snyk.
- IaC scanning: checkov/tfsec/ansible-lint and ansible-review for policy compliance.
- SBOM: generate with syft and store alongside artifact.
- Vulnerability policy: define severity thresholds that fail the pipeline.
- Signed artifacts: sign with KMS or GPG and validate signature in deploy stage.

Compliance and evidence:
- Store pipeline run metadata and artifact manifests in a secure location for audit.
- Keep CloudTrail and Jenkins audit logs for retention window required by policy.

---

Observability, logging & health checks
- Health checks:
  - Define HTTP/GRPC health endpoints for smoke tests.
  - Use readiness/liveness semantics where appropriate.
- Telemetry:
  - Export application metrics to Prometheus, send logs to central ELK/CloudWatch.
  - Post-deploy automated checks for error rate, latency, 5xx ratio.
- Dashboards & alerts:
  - Dashboards for deployment frequency, failure rate, lead time, MTTR.
  - Configure alerts to Slack/email on deployment failure or degraded metrics.
- Deployment metrics in Jenkins:
  - Instrument deployment steps, durations, and outcomes.

---

Rollback & disaster recovery
- Immutable artifact + current symlink pattern enables instant rollback by pointing symlink to previous release.
- For DB migrations:
  - Use backward-compatible migrations or have a rollback plan that includes DB restore from backup.
  - Take pre-migration backups automatically in pipeline when migrations are about to run in production.
- Automated rollback policy:
  - Detect failed health checks or increase in error metrics and run an automated rollback job that points `current` to the last known-good release and restarts services.
- Manual rollback runbook:
  - Steps include: stop traffic to failing instance, re-deploy previous artifact, run DB rollback if needed, run smoke tests, re-enable traffic.

---

Recommended Jenkins plugins & ecosystem
- Pipeline (workflow-aggregator)
- GitHub Branch Source / Bitbucket Branch Source
- Credentials Binding
- Amazon Web Services Credentials / AWS Steps (if used)
- SSH Agent
- Role-based Authorization Strategy
- Blue Ocean (optional)
- Pipeline: Multibranch
- GitHub PR Decoration
- HashiCorp Vault plugin (if using Vault)
- Job DSL or Shared Library for pipeline code reuse
- Slack Notification Plugin (or other notification integration)

Jenkins infrastructure
- Run Jenkins in a secure subnet, behind an ALB with TLS, and with authentication via SSO (OIDC).
- Use ephemeral agents (Kubernetes or cloud agents) to reduce standing credentials on agents.
- Backup configuration and credentials, enable master/agent encryption (JNLP with TLS).

---

Appendices

A. Example commands Jenkins uses (conceptual)
- aws s3 cp dist/artifact.tar.gz s3://my-deploy-bucket/artifacts/my-app/1.2.3/artifact.tar.gz
- ansible-playbook -i inventories/prod playbooks/deploy-s3.yml -e "s3_bucket=my-deploy-bucket s3_key=artifacts/my-app/1.2.3/artifact.tar.gz"

B. Environment variables commonly used (Jenkins)
- AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY (prefer IAM role instead)
- ANSIBLE_PRIVATE_KEY_FILE (injected via withCredentials)
- BUILD_NUMBER, GIT_COMMIT, BRANCH_NAME (standard Jenkins env vars)
- DEPLOY_ENV (staging/prod)

C. Checklist before enabling automated production deploys
- [ ] Branch protection in GitHub
- [ ] Unit + integration + security scans passing
- [ ] Artifact stored with manifest and SBOM
- [ ] Manual approval gate configured with limited approvers
- [ ] Observability & alerting active
- [ ] Rollback runbook validated in drills
- [ ] IAM least privilege applied
- [ ] Logging & audit enabled

---

What's next
- I created the expanded corporate-grade guide and example Jenkinsfile ready to drop into a multibranch pipeline. Next I can:
  - Generate a ready-to-run Jenkins Job DSL or a multibranch pipeline configuration for your Jenkins instance.
  - Scaffold a reusable Ansible role (roles/app_deploy) with molecule tests.
  - Produce a production-ready deployment playbook variant (blue/green + health checks + automated rollback).
Pick one and I'll produce the code and files you can copy into your repo.
```
